{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESS THE TWEET MESSAGES\n",
    "\n",
    "- Language Identification\n",
    "- Case conversion\n",
    "- Clen Text From tags (e.g. Html)\n",
    "- Remove Stopwords \n",
    "- Expand Contractions\n",
    "- Encode Relevant Emoticons and Punctuation\n",
    "- Remove Special Characters\n",
    "- Correcting Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE FUNCTIONS\n",
    "\n",
    "- All functions are modular, the algorithm can be improved by choosing the right mix of feature selection and data cleansing.\n",
    "- The order is important! The cleaning something (e.g. 1 word character) before processing symbols or special characters (e.g. emoticons) before converting them would compromise the algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a DICT with the important emticons and punctuactions\n",
    "\n",
    "# Emoticons\n",
    "EMOTICONS = \\\n",
    "    [   ('__EMJ_SMILEY',   [':-)', ':)', '(:', '(-:', ] )  ,\\\n",
    "        ('__EMJ_LAUGH',        [':-D', ':D', 'X-D', 'XD', 'xD', ':P', ':p' ] )    ,\\\n",
    "        ('__EMJ_LOVE',     ['<3', ':\\*', ] )   ,\\\n",
    "        ('__EMJ_WINK',     [';-)', ';)', ';-D', ';D', '(;', '(-;', ] ) ,\\\n",
    "        ('__EMJ_FROWN',        [':-(', ':(', ] )   ,\\\n",
    "        ('__EMJ_CRY',      [':,(', ':\\'(', ':\"(', ':(('] ) ,\\\n",
    "    ]\n",
    "\n",
    "# Punctuations\n",
    "PUNCTUATIONS = \\\n",
    "    [   ('_P_EXCL',['!', '¡', ] ),\\\n",
    "        ('_P_QUES',['?', '¿', ] ),\\\n",
    "        ('_P_EL',['...', '…', ] ),\\\n",
    "    ]\n",
    "    \n",
    "    \n",
    "CONTRACTION_DIC = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODULAR FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check language \n",
    "#It returns the language code\n",
    "from langdetect import detect\n",
    "def check_language(msg):\n",
    "    try:\n",
    "        language = detect(msg)\n",
    "    except:\n",
    "        #if only punctuations are detected, assume it is an emoticon with sentiment polarity\n",
    "        #Hence, do not drop it \n",
    "        language = 'en'\n",
    "    return language\n",
    "\n",
    "#Case Conversion - Gengeral String\n",
    "def lower_case(msg):\n",
    "    msg = msg.lower()\n",
    "    return msg\n",
    "\n",
    "#Case Conversion - Toknes\n",
    "def lower_case_tokens(tokens):\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "#Remove URLS\n",
    "def remove_urls(msg):\n",
    "    msg = re.sub(r\"http\\S+\", \"\", msg)\n",
    "    return msg\n",
    "\n",
    "#Remove User Account\n",
    "def remove_user_account(msg):\n",
    "    msg = re.sub(r\"@[^\\s]+[\\s]?\", \"\", msg)\n",
    "    return msg\n",
    "\n",
    "#Remove Numbers\n",
    "def remove_numbers(msg):\n",
    "    msg = re.sub(r\"\\d+\", \"\", msg)\n",
    "    return msg\n",
    "\n",
    "#Remove RT\n",
    "def remove_retweet(msg):\n",
    "    msg = re.sub(r'rt', \"\", msg)\n",
    "    return msg\n",
    "\n",
    "#Remove Hashtag\n",
    "def remove_hashtag(msg):\n",
    "    msg = re.sub(r\"#[^\\s]+[\\s]?\", \"\", msg)\n",
    "    return msg\n",
    "\n",
    "#TOKENIZE\n",
    "#It's expecially tailored for tweets (different from word_tokenize)\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "def tokenizer (msg):\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokens = tknzr.tokenize(msg)\n",
    "    return tokens\n",
    "\n",
    "#EXPAND CONTRACTIONS\n",
    "def expand_contractions(s, CONTRACTION_MAP = CONTRACTION_DIC):\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(CONTRACTION_MAP.keys()))\n",
    "    def replace(match):\n",
    "        return CONTRACTION_MAP[match.group(0)]\n",
    "    \n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "#REMOVE STOP-WORDS\n",
    "#Keep Meaningful stopwords\n",
    "meaningful_stops = ['no', 'nor', 'not']\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords = [word for word in stopwords if word not in meaningful_stops]\n",
    "def remove_stopwords(tokens):\n",
    "    cleaned_tokens = [token for token in tokens if token not in stopwords]\n",
    "    return cleaned_tokens\n",
    "\n",
    "#CORRECT REPEATING CHARACTERS\n",
    "from nltk.corpus import wordnet\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    #it checks in a dictionary of english words when to stop correcting\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "#STEMMING\n",
    "#from nltk.stem import PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.regexp import RegexpStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "def stemming(tokens): #TODO: Add a dictionary\n",
    "    #st = LancasterStemmer()\n",
    "    st = PorterStemmer()\n",
    "    #st = RegexpStemmer()\n",
    "    #st = SnowballStemmer('english')\n",
    "    def stemmer_token(old_word):\n",
    "        stem_token = st.stem(old_word)\n",
    "        return stem_token\n",
    "    stem_tokens = [stemmer_token(tkn) for tkn in tokens]\n",
    "    return stem_tokens\n",
    "\n",
    "\n",
    "#LEMMING\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemming(tokens):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    def lemmer_token(old_word):\n",
    "        lem_token = wnl.lemmatize(old_word)\n",
    "        return lem_token\n",
    "    \n",
    "    lem_tokens = [lemmer_token(tkn) for tkn in tokens]\n",
    "    return lem_tokens\n",
    "\n",
    "        \n",
    "\n",
    "#CONVERT SPECIAL CHARACTERS FOR SENTIMENT ANALYSIS\n",
    "def replace_special_characters(tokens):\n",
    "    \n",
    "    def replace(token, punctuations=PUNCTUATIONS):\n",
    "        d = {r: punct for punct, replacement in punctuations for r in replacement}\n",
    "        for punct, replacement in punctuations:\n",
    "            pattern = \"(\" + \"|\".join(map(re.escape, replacement)) + \")\"\n",
    "            token = re.sub(pattern, lambda m: d[m.group()], token)\n",
    "        return token\n",
    "    \n",
    "    punct_tokens = [replace(tkn) for tkn in tokens]\n",
    "    return punct_tokens\n",
    "        \n",
    "\n",
    "\n",
    "#EMOTICONS - Keep only meaningful ones\n",
    "def replace_emojis(tokens):\n",
    "    \n",
    "    def replace(token, emoticons=EMOTICONS):\n",
    "        d = {r: emote for emote, replacement in emoticons for r in replacement}\n",
    "        for emote, replacement in emoticons:\n",
    "            pattern = \"(\" + \"|\".join(map(re.escape, replacement)) + \")\"\n",
    "            token = re.sub(pattern, lambda m: d[m.group()], token)\n",
    "        return token\n",
    "    \n",
    "    emj_tokens = [replace(tkn) for tkn in tokens]\n",
    "    return emj_tokens\n",
    "\n",
    "\n",
    "#REMOVE SPECIAL CHARACTERS NOT RELEVANT from tokens\n",
    "#TODO: It may need more research about which character to delete and which to keep\n",
    "# Details\n",
    "# [^ - start of a *negated character class\n",
    "# \\s - whitespace\n",
    "# \\w - word char (letter, digit or/and _)\n",
    "# ' - a single quote\n",
    "# & - a & symbol -% - a % symbol\n",
    "# - - a hyphen (since it is at the end, it will be parsed as a literal -)\n",
    "# ] - end of the character class.\n",
    "def remove_special(tokens):\n",
    "    def remove(token):\n",
    "        pattern = re.compile(r\"[^\\s\\w'-]\")\n",
    "        token = pattern.sub('', token)\n",
    "        return token\n",
    "    \n",
    "    clean_tokens = [remove(tkn) for tkn in tokens]\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "#REMOVE 1 LETTER WORDS\n",
    "def remove_one_letter(tokens):\n",
    "    def remove(token):\n",
    "        if len(token)>1:\n",
    "            return token\n",
    "        pass\n",
    "    clean_tokens = [remove(tkn) for tkn in tokens]\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "#CLEAN NULL TOKEN\n",
    "def filter_null_token(tokens):\n",
    "    tokens = list(filter(None, tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Text Normalization for SENTIMENT ANALYSIS\n",
    "def text_normalization_sentiment (msg):\n",
    "    #Convert everything in lowcase\n",
    "    #msg = lower_case(msg)\n",
    "    #Remove Urls\n",
    "    msg = remove_urls(msg)\n",
    "    #Remove 'rt' (retweet) TODO: It needs to preserve a STRING!\n",
    "    msg = remove_retweet(msg)\n",
    "    #Remove '@' (mentions)\n",
    "    msg = remove_user_account(msg)\n",
    "    #Remove Hashtag\n",
    "    msg = remove_hashtag(msg)\n",
    "    #Remove Numbers\n",
    "    msg = remove_numbers(msg)\n",
    "    #Expand Contractions\n",
    "    msg = expand_contractions(msg)\n",
    "    \n",
    "    ######### TOKENIZER ## NOW WE DEAL WITH -TOKENS- ########\n",
    "    msg_tkn = tokenizer(msg)\n",
    "    #Encode Relevant Emoticons\n",
    "    msg_tkn = replace_emojis(msg_tkn)\n",
    "    #Lowcase after tokenization and emotijs\n",
    "    msg_tkn = lower_case_tokens(msg_tkn)\n",
    "    #Correcting repeating characters\n",
    "    msg_tkn = remove_repeated_characters(msg_tkn)\n",
    "    #Remove stop-words\n",
    "    msg_tkn = remove_stopwords(msg_tkn)\n",
    "    \n",
    "    #NB. Apply Lemmatization before, and after Stemming!\n",
    "    #Lemmatization\n",
    "    msg_tkn = lemming(msg_tkn)\n",
    "    #Stemming (which algorithm?)\n",
    "    msg_tkn = stemming(msg_tkn)\n",
    "   \n",
    "        \n",
    "    #Special Character (punctuation)\n",
    "    msg_tkn = replace_special_characters(msg_tkn)\n",
    "    \n",
    "    #Remove Special Character left\n",
    "    msg_tkn = remove_special(msg_tkn)\n",
    "    \n",
    "    #Remove 1 letter tokens\n",
    "    mgs_tkn = remove_one_letter(msg_tkn)\n",
    "    \n",
    "    #Attention: Clean from Null tokens\n",
    "    msg_tkn = filter_null_token(msg_tkn)\n",
    "\n",
    "    #Join the tokens back (the classifier tfidf needs a string)\n",
    "    msg_tkn = \" \".join(msg_tkn)\n",
    "    \n",
    "    return msg_tkn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABELED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Labeled Dataset: (1578612, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Juuuuuuuuuuuuuuuuussssst Chillin!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Sunny Again        Work Tomorrow  :-|  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>handed in my uniform today . i miss you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>hmmmm.... i wonder how she my number @-)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ...\n",
       "5          0                  or i just worry too much?        \n",
       "6          1                 Juuuuuuuuuuuuuuuuussssst Chillin!!\n",
       "7          0         Sunny Again        Work Tomorrow  :-|  ...\n",
       "8          1        handed in my uniform today . i miss you ...\n",
       "9          1           hmmmm.... i wonder how she my number @-)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import LABELED DATASET (BINARY) OF TWEETS\n",
    "#2 labels dataset: Positive - Negative\n",
    "\n",
    "df_labeled = pd.read_csv('D:/NLP Project - Resources/Trainded Dataset - Sentiment.csv', error_bad_lines=False)\n",
    "df_labeled = df_labeled[['Sentiment','SentimentText']]\n",
    "df_labeled.dropna()\n",
    "print('Shape Labeled Dataset:', df_labeled.shape)\n",
    "df_labeled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive labeled tweets: 790177\n",
      "Negative labeled tweets: 788435\n"
     ]
    }
   ],
   "source": [
    "#Number of positive and negative tweets.\n",
    "\n",
    "negative = df_labeled.ix[(df_labeled['Sentiment']==0)]\n",
    "positive = df_labeled.ix[(df_labeled['Sentiment']==1)]\n",
    "print('Positive labeled tweets:', positive.shape[0])\n",
    "print('Negative labeled tweets:', negative.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 9575.795402526855 seconds to check language---\n"
     ]
    }
   ],
   "source": [
    "#Keep only english messages\n",
    "#This would prevent unfiltered non-english messages to reduce the pefromances of the model\n",
    "#CHECK THE LANGUAGE\n",
    "start_time = time.time()\n",
    "\n",
    "df_labeled['language'] = df_labeled.SentimentText.apply(check_language)\n",
    "\n",
    "print(\"--- %s seconds to check language---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Labeled Dataset - Only English texts: (1470739, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>or i just worry too much?</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Sunny Again        Work Tomorrow  :-|  ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>handed in my uniform today . i miss you ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>hmmmm.... i wonder how she my number @-)</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>I must think about positive..</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiment                                      SentimentText language\n",
       "0           0                       is so sad for my APL frie...       en\n",
       "1           0                     I missed the New Moon trail...       en\n",
       "2           1                            omg its already 7:30 :O       en\n",
       "3           0            .. Omgaga. Im sooo  im gunna CRy. I'...       en\n",
       "4           0           i think mi bf is cheating on me!!!   ...       en\n",
       "5           0                  or i just worry too much?               en\n",
       "7           0         Sunny Again        Work Tomorrow  :-|  ...       en\n",
       "8           1        handed in my uniform today . i miss you ...       en\n",
       "9           1           hmmmm.... i wonder how she my number @-)       en\n",
       "10          0                      I must think about positive..       en"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter Only English Tweets\n",
    "df_labeled = df_labeled[df_labeled.language == 'en']\n",
    "print('Shape Labeled Dataset - Only English texts:', df_labeled.shape)\n",
    "df_labeled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Store the filtered labeled dataset\n",
    "df_labeled.to_csv('D:/NLP Project - Resources/Trainded Dataset - EnglishFiltered.csv',\n",
    "                  encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ConuntVectorizer: It learns the vocaboluary of the corpus and extracts word count features (Bag of Words).\n",
    "\n",
    "- Pipeline: connects a series of steps into one ojbect. We can use it to merge the feature extraction and classification in one operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import sklearn.metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import LABELED DATASET (BINARY) OF TWEETS\n",
    "#2 labels dataset: Positive - Negative\n",
    "df_labeled = pd.read_csv('D:/NLP Project - Resources/Trainded Dataset - EnglishFiltered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3755.5590500831604 seconds to normalize the labeled dataset---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Assign the Tweets to X and the Sentiment to Y\n",
    "#TWEET (1,5 milion of tweets)\n",
    "df_labeled = df_labeled.loc[2:1450000]\n",
    "#Normalize tweets for Sentiment Analysis\n",
    "df_labeled['Normalized Text'] = df_labeled.SentimentText.apply(text_normalization_sentiment)\n",
    "#Prevent NaN after normalization (it can affect some embedding procedures)\n",
    "df_labeled = df_labeled.dropna()\n",
    "\n",
    "print(\"--- %s seconds to normalize the labeled dataset---\" % (time.time() - start_time))\n",
    "\n",
    "#TWEETS NORMALIZED\n",
    "X = df_labeled[['Normalized Text']]\n",
    "#SENTIMENT LABEL\n",
    "y = df_labeled[['Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_labeledeled.to_csv('D:/NLP Project - Resources/Trained Dataset - EnglishFiltered - NORMALIZED.csv',\n",
    "                 encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Algorithm (linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example.\n",
    "For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does\n",
    "a better job of getting all the training points classified correctly. Conversely,\n",
    "a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane,\n",
    "even if that hyperplane misclassifies more points.\n",
    "For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset in testing and training - particular attention to the index\n",
    "X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X.iloc[:,0], y.iloc[:,0],\n",
    "                                                                             test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 944.1860303878784 seconds to train SVM algorithm---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#C=0.5 --> Small C values increase the separation margin on the hyperplane. It is beneficial for this model, because\n",
    "#the models need to be able to generalize in a microblog context\n",
    "vec_clf = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,3))),\n",
    "                   ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                   ('clf', LinearSVC(C=0.5))])\n",
    "#Fit the SVM model\n",
    "vec_clf.fit(X_train, y_train)\n",
    "#Save the classifier\n",
    "joblib.dump(vec_clf, 'svmClassifier v02.pk1', compress=3)\n",
    "\n",
    "print(\"--- %s seconds to train SVM algorithm---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.83      0.81    145285\n",
      "          1       0.82      0.79      0.81    144715\n",
      "\n",
      "avg / total       0.81      0.81      0.81    290000\n",
      "\n",
      "Distance boundaries\n",
      "3.8552277354\n",
      "-8.88838657272\n"
     ]
    }
   ],
   "source": [
    "####Evaluation Classifier: Linear Support Vector#####\n",
    "y_pred = vec_clf.predict(X_test)\n",
    "y_confidence = vec_clf.decision_function(X_test)\n",
    "\n",
    "#RESULTS\n",
    "print (sklearn.metrics.classification_report(y_test, y_pred))\n",
    "print ('Distance boundaries')\n",
    "print (y_confidence.max())\n",
    "print (y_confidence.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAIVE BAYES CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset in testing and training - particular attention to the index for Random Forest\n",
    "X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X.iloc[:,0], y.iloc[:,0],\n",
    "                                                                             test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 394.0458130836487 seconds to train Naive Bayes Classifier---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "naiv_cl = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,3))),\n",
    "                   ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                   ('clf', MultinomialNB())])\n",
    "\n",
    "naiv_cl = naiv_cl.fit(X_train, y_train)\n",
    "#Save the classifier\n",
    "joblib.dump(naiv_cl, 'multinomialNBCassifier V02.pk1', compress=3)\n",
    "\n",
    "print(\"--- %s seconds to train Naive Bayes Classifier---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.82      0.80    145285\n",
      "          1       0.81      0.76      0.78    144715\n",
      "\n",
      "avg / total       0.79      0.79      0.79    290000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####PREDICTION: Classifier: MultinomialNB#####\n",
    "y_pred = naiv_cl.predict(X_test)\n",
    "#Percentage of belonging to each class\n",
    "y_confidence = naiv_cl.predict_proba(X_test)\n",
    "\n",
    "#RESULTS\n",
    "print (sklearn.metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset in testing and training - particular attention to the index for Random Forest\n",
    "X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X.iloc[:,0], y.iloc[:,0],\n",
    "                                                                             test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 120492.02273583412 seconds to train Random Forest Classifier---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "randfor_cl = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,3))),\n",
    "                       ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', RandomForestClassifier())])\n",
    "\n",
    "randfor_cl = randfor_cl.fit(X_train, y_train)\n",
    "#Save the classifier\n",
    "joblib.dump(randfor_cl, 'randomForestCassifier.pk1', compress=3)\n",
    "\n",
    "print(\"--- %s seconds to train Random Forest Classifier---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.80      0.77    145285\n",
      "          1       0.78      0.72      0.75    144715\n",
      "\n",
      "avg / total       0.76      0.76      0.76    290000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####PREDICTION: Classifier: MultinomialNB#####\n",
    "y_pred = randfor_cl.predict(X_test)\n",
    "\n",
    "#RESULTS\n",
    "print (sklearn.metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "#Percentage of belonging to each class\n",
    "y_confidence = randfor_cl.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAXIMUM ENTROPY (LOGISTIC REGRESSION) CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset in testing and training - particular attention to the index for Random Forest\n",
    "X_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(X.iloc[:,0], y.iloc[:,0],\n",
    "                                                                             test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 512.7569930553436 seconds to train Logistic Regression Classifier---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "logistic_cl = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,3))),\n",
    "                       ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                       ('clf', LogisticRegression())])\n",
    "\n",
    "logistic_cl = logistic_cl.fit(X_train, y_train)\n",
    "#Save the classifier\n",
    "joblib.dump(randfor_cl, 'logisticCassifier.pk1', compress=3)\n",
    "\n",
    "print(\"--- %s seconds to train Logistic Regression Classifier---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.82      0.81    145285\n",
      "          1       0.82      0.79      0.80    144715\n",
      "\n",
      "avg / total       0.81      0.81      0.81    290000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####PREDICTION: Classifier: LogistiRegression#####\n",
    "y_pred = logistic_cl.predict(X_test)\n",
    "\n",
    "#RESULTS\n",
    "print (sklearn.metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "#Percentage of belonging to each class\n",
    "y_confidence = randfor_cl.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}